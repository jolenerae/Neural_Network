{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24339606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1, Training Loss: 1.8522628733852524, Validation Loss: 1.7866518717662545\n",
      "Epoch 2, Training Loss: 1.679488136908986, Validation Loss: 1.6518771435804427\n",
      "Epoch 3, Training Loss: 1.5929192552010727, Validation Loss: 1.5939091147890516\n",
      "Epoch 4, Training Loss: 1.5391550693600078, Validation Loss: 1.5611265548475228\n",
      "Epoch 5, Training Loss: 1.495851471903651, Validation Loss: 1.518199887245324\n",
      "Epoch 6, Training Loss: 1.464952661072691, Validation Loss: 1.4851404284216037\n",
      "Epoch 7, Training Loss: 1.4379180180492686, Validation Loss: 1.447753946112979\n",
      "Epoch 8, Training Loss: 1.4175125457076376, Validation Loss: 1.4727648086608596\n",
      "Epoch 9, Training Loss: 1.3938566452235133, Validation Loss: 1.4673336300120992\n",
      "Epoch 10, Training Loss: 1.3819144067818507, Validation Loss: 1.5025643197594174\n",
      "Epoch 11, Training Loss: 1.361192786998586, Validation Loss: 1.4939345611128838\n",
      "Epoch 12, Training Loss: 1.3450230546004398, Validation Loss: 1.4865230971081242\n",
      "Epoch 13, Training Loss: 1.3305255683415649, Validation Loss: 1.4451380307507362\n",
      "Epoch 14, Training Loss: 1.3143184221883826, Validation Loss: 1.5011499015388974\n",
      "Epoch 15, Training Loss: 1.301068783060574, Validation Loss: 1.4938946749754012\n",
      "Epoch 16, Training Loss: 1.290818345148511, Validation Loss: 1.476905988280181\n",
      "Epoch 17, Training Loss: 1.2768127501815791, Validation Loss: 1.4678861962002554\n",
      "Epoch 18, Training Loss: 1.2645371976945954, Validation Loss: 1.5246424052366025\n",
      "Epoch 19, Training Loss: 1.2533262918498724, Validation Loss: 1.5112794030244183\n",
      "Epoch 20, Training Loss: 1.2468237255851513, Validation Loss: 1.4676947164687382\n",
      "Epoch 21, Training Loss: 1.233553710649771, Validation Loss: 1.446965224803633\n",
      "Epoch 22, Training Loss: 1.2236832680224357, Validation Loss: 1.4957274851525666\n",
      "Epoch 23, Training Loss: 1.2154849473262561, Validation Loss: 1.48794243024413\n",
      "Epoch 24, Training Loss: 1.2032285175089643, Validation Loss: 1.5153947916759807\n",
      "Epoch 25, Training Loss: 1.2005984556107294, Validation Loss: 1.478208684997194\n",
      "Epoch 26, Training Loss: 1.1933138585293979, Validation Loss: 1.521903776439132\n",
      "Epoch 27, Training Loss: 1.1881177357180202, Validation Loss: 1.5102427142440893\n",
      "Epoch 28, Training Loss: 1.178715871891911, Validation Loss: 1.4897156027471943\n",
      "Epoch 29, Training Loss: 1.1713042715041453, Validation Loss: 1.515266569556704\n",
      "Epoch 30, Training Loss: 1.160059866408719, Validation Loss: 1.5220787039228305\n",
      "Epoch 31, Training Loss: 1.1515087613673098, Validation Loss: 1.512075659955383\n",
      "Epoch 32, Training Loss: 1.1468080176727604, Validation Loss: 1.5611646232331635\n",
      "Epoch 33, Training Loss: 1.1388081019451708, Validation Loss: 1.5840360370411235\n",
      "Epoch 34, Training Loss: 1.133162079292913, Validation Loss: 1.549316717181236\n",
      "Epoch 35, Training Loss: 1.1271497434390378, Validation Loss: 1.5417164514778525\n",
      "Epoch 36, Training Loss: 1.1195758027477982, Validation Loss: 1.623938384709085\n",
      "Epoch 37, Training Loss: 1.1145945907740535, Validation Loss: 1.590242862701416\n",
      "Epoch 38, Training Loss: 1.1123037971582663, Validation Loss: 1.5648862909359538\n",
      "Epoch 39, Training Loss: 1.105081666549555, Validation Loss: 1.5932349198183435\n",
      "Epoch 40, Training Loss: 1.1017464489359, Validation Loss: 1.61172104755025\n",
      "Epoch 41, Training Loss: 1.0879644701984137, Validation Loss: 1.561285369335466\n",
      "Epoch 42, Training Loss: 1.085954619877374, Validation Loss: 1.6120540624970843\n",
      "Epoch 43, Training Loss: 1.0848866108256872, Validation Loss: 1.6217604719909133\n",
      "Epoch 44, Training Loss: 1.083478715585832, Validation Loss: 1.6098624152742373\n",
      "Epoch 45, Training Loss: 1.0695145038653537, Validation Loss: 1.6431253480303818\n",
      "Epoch 46, Training Loss: 1.0696501228812745, Validation Loss: 1.655295137007525\n",
      "Epoch 47, Training Loss: 1.0680080797329978, Validation Loss: 1.677394950086144\n",
      "Epoch 48, Training Loss: 1.0579840881060265, Validation Loss: 1.7183819063909493\n",
      "Epoch 49, Training Loss: 1.0523840196181802, Validation Loss: 1.6849650630525723\n",
      "Epoch 50, Training Loss: 1.0460404840105377, Validation Loss: 1.6654566347978677\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        return self.network(x)\n",
    "\n",
    "dataset = CIFAR10(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "input_size = 32*32*3\n",
    "input_channels = 3\n",
    "\n",
    "hidden_sizes = [512, 256]\n",
    "output_size = 10\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Validation Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {total_train_loss/len(train_loader)}, Validation Loss: {total_val_loss/len(val_loader)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae0866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1, Training Loss: 1.7468563402131168, Validation Loss: 1.4979702811332265\n",
      "Epoch 2, Training Loss: 1.4515307525751349, Validation Loss: 1.4965566396713257\n",
      "Epoch 3, Training Loss: 1.3461069717590235, Validation Loss: 1.332583344666062\n",
      "Epoch 4, Training Loss: 1.2700845391172506, Validation Loss: 1.2734183862710455\n",
      "Epoch 5, Training Loss: 1.211539069176059, Validation Loss: 1.2592675856723907\n",
      "Epoch 6, Training Loss: 1.1600594520992478, Validation Loss: 1.1989805804696052\n",
      "Epoch 7, Training Loss: 1.1192774033410886, Validation Loss: 1.2481766502568676\n",
      "Epoch 8, Training Loss: 1.0764984884580655, Validation Loss: 1.1971784420074172\n",
      "Epoch 9, Training Loss: 1.047777524168981, Validation Loss: 1.1843776190356843\n",
      "Epoch 10, Training Loss: 1.0196487727365176, Validation Loss: 1.1610776823797044\n",
      "Epoch 11, Training Loss: 0.9840894155071912, Validation Loss: 1.2242045630315306\n",
      "Epoch 12, Training Loss: 0.9588973921456909, Validation Loss: 1.1941987097642983\n",
      "Epoch 13, Training Loss: 0.9327933677732309, Validation Loss: 1.1739300246451312\n",
      "Epoch 14, Training Loss: 0.9090732143081158, Validation Loss: 1.193869653780749\n",
      "Epoch 15, Training Loss: 0.8811893710534286, Validation Loss: 1.235055776538363\n",
      "Epoch 16, Training Loss: 0.8587269187820831, Validation Loss: 1.1863136895143302\n",
      "Epoch 17, Training Loss: 0.8381049633873382, Validation Loss: 1.21283146378341\n",
      "Epoch 18, Training Loss: 0.8172008255609148, Validation Loss: 1.2190734732682538\n",
      "Epoch 19, Training Loss: 0.7997868867020448, Validation Loss: 1.2512676267866876\n",
      "Epoch 20, Training Loss: 0.7820693275440477, Validation Loss: 1.2475881728397054\n",
      "Epoch 21, Training Loss: 0.7634775950371498, Validation Loss: 1.2422878442296557\n",
      "Epoch 22, Training Loss: 0.7489824264542635, Validation Loss: 1.2746966746961994\n",
      "Epoch 23, Training Loss: 0.7336817952488531, Validation Loss: 1.3180123985193337\n",
      "Epoch 24, Training Loss: 0.7142526050069777, Validation Loss: 1.3191117898673768\n",
      "Epoch 25, Training Loss: 0.6998144880032489, Validation Loss: 1.3202643128717022\n",
      "Epoch 26, Training Loss: 0.6844026462228567, Validation Loss: 1.395734298760724\n",
      "Epoch 27, Training Loss: 0.6686074940571144, Validation Loss: 1.3777450482556775\n",
      "Epoch 28, Training Loss: 0.6555327846170235, Validation Loss: 1.3987497110275706\n",
      "Epoch 29, Training Loss: 0.642011051048348, Validation Loss: 1.4108026035272392\n",
      "Epoch 30, Training Loss: 0.624232627859163, Validation Loss: 1.4800155812008366\n",
      "Epoch 31, Training Loss: 0.618251532390343, Validation Loss: 1.5278501958604072\n",
      "Epoch 32, Training Loss: 0.6018292486095733, Validation Loss: 1.5137924855681741\n",
      "Epoch 33, Training Loss: 0.5913590486899453, Validation Loss: 1.5352751202644057\n",
      "Epoch 34, Training Loss: 0.5811178082469176, Validation Loss: 1.5361487345331033\n",
      "Epoch 35, Training Loss: 0.5649510978190884, Validation Loss: 1.5975010725343304\n",
      "Epoch 36, Training Loss: 0.5610720888256773, Validation Loss: 1.5836990969196247\n",
      "Epoch 37, Training Loss: 0.5463459604309228, Validation Loss: 1.6646780174249296\n",
      "Epoch 38, Training Loss: 0.5371935114783434, Validation Loss: 1.6602609153765782\n",
      "Epoch 39, Training Loss: 0.5275753880212217, Validation Loss: 1.6804870025367493\n",
      "Epoch 40, Training Loss: 0.5171322071793746, Validation Loss: 1.7897124020916642\n",
      "Epoch 41, Training Loss: 0.5080310630942374, Validation Loss: 1.7562664788999376\n",
      "Epoch 42, Training Loss: 0.4920372623095041, Validation Loss: 1.7556911881562252\n",
      "Epoch 43, Training Loss: 0.49299626969537075, Validation Loss: 1.844452417200538\n",
      "Epoch 44, Training Loss: 0.4777694629570145, Validation Loss: 1.8357557092502619\n",
      "Epoch 45, Training Loss: 0.47384318694546446, Validation Loss: 1.9266670266534114\n",
      "Epoch 46, Training Loss: 0.4601080374422866, Validation Loss: 1.9139460575808385\n",
      "Epoch 47, Training Loss: 0.4550271457658224, Validation Loss: 1.9024127057403515\n",
      "Epoch 48, Training Loss: 0.4464074488843089, Validation Loss: 1.9615903010793552\n",
      "Epoch 49, Training Loss: 0.43952333868375976, Validation Loss: 2.022388945339592\n",
      "Epoch 50, Training Loss: 0.43355680498550697, Validation Loss: 2.087641657917363\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, stride=2)\n",
    "        )\n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_model(x)\n",
    "\n",
    "dataset = CIFAR10(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "input_size = 32*32*3\n",
    "input_channels = 3\n",
    "\n",
    "hidden_sizes = [512, 256]\n",
    "output_size = 10\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = LeNet(input_channels)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Validation Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {total_train_loss/len(train_loader)}, Validation Loss: {total_val_loss/len(val_loader)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class FlexNet(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(FlexNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Size reduces to 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Size reduces to 8x8\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 512),  # Adjusted for the final size of the feature maps\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def train_model(dataset_class, input_channels):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = dataset_class(root='.', train=True, download=True, transform=transform)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = FlexNet(input_channels=input_channels)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Training Loss: {total_train_loss / len(train_loader)}, Validation Loss: {total_val_loss / len(val_loader)}')\n",
    "\n",
    "train_model(CIFAR10, 3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02885a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
