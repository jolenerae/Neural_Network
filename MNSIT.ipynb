{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b19c00bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2975421574654199, Validation Loss: 0.15366305049231394\n",
      "Epoch 2, Training Loss: 0.12488181145737054, Validation Loss: 0.12079703886596922\n",
      "Epoch 3, Training Loss: 0.08527204241593682, Validation Loss: 0.11255587978991977\n",
      "Epoch 4, Training Loss: 0.06345646022809327, Validation Loss: 0.09809952446487435\n",
      "Epoch 5, Training Loss: 0.0501909196491871, Validation Loss: 0.1089514776633339\n",
      "Epoch 6, Training Loss: 0.04110282563563, Validation Loss: 0.10506006036272045\n",
      "Epoch 7, Training Loss: 0.033241383601354875, Validation Loss: 0.0973052381242714\n",
      "Epoch 8, Training Loss: 0.028662993890535823, Validation Loss: 0.10053190004080224\n",
      "Epoch 9, Training Loss: 0.024098262809313976, Validation Loss: 0.11193243389417593\n",
      "Epoch 10, Training Loss: 0.021398113057965976, Validation Loss: 0.10931122344720975\n",
      "Epoch 11, Training Loss: 0.01819633591454814, Validation Loss: 0.12424562143873931\n",
      "Epoch 12, Training Loss: 0.01836483513767758, Validation Loss: 0.11629678114746261\n",
      "Epoch 13, Training Loss: 0.014658187248363378, Validation Loss: 0.13732978331985907\n",
      "Epoch 14, Training Loss: 0.013813336104949009, Validation Loss: 0.14070738917540593\n",
      "Epoch 15, Training Loss: 0.014623445638240232, Validation Loss: 0.13915281053150563\n",
      "Epoch 16, Training Loss: 0.011908391158023881, Validation Loss: 0.14223005009859827\n",
      "Epoch 17, Training Loss: 0.011385751134744878, Validation Loss: 0.12879239945736712\n",
      "Epoch 18, Training Loss: 0.010700843806710823, Validation Loss: 0.15135639038424478\n",
      "Epoch 19, Training Loss: 0.009148565827387777, Validation Loss: 0.16003935280331252\n",
      "Epoch 20, Training Loss: 0.010840639426735863, Validation Loss: 0.16084966644512724\n",
      "Epoch 21, Training Loss: 0.008972887526780188, Validation Loss: 0.15984174952898642\n",
      "Epoch 22, Training Loss: 0.010469247358706656, Validation Loss: 0.19059535944551523\n",
      "Epoch 23, Training Loss: 0.009031957354789856, Validation Loss: 0.1712107221265261\n",
      "Epoch 24, Training Loss: 0.00770259157977998, Validation Loss: 0.14737332286405622\n",
      "Epoch 25, Training Loss: 0.008129805495734315, Validation Loss: 0.1614117518327536\n",
      "Epoch 26, Training Loss: 0.008922490591284119, Validation Loss: 0.1959738149643799\n",
      "Epoch 27, Training Loss: 0.010724555711954051, Validation Loss: 0.17894862994625518\n",
      "Epoch 28, Training Loss: 0.006723425592782501, Validation Loss: 0.18110653778085348\n",
      "Epoch 29, Training Loss: 0.008340349125972263, Validation Loss: 0.16960116329571534\n",
      "Epoch 30, Training Loss: 0.005331955200797766, Validation Loss: 0.1979996358181191\n",
      "Epoch 31, Training Loss: 0.009372416213077581, Validation Loss: 0.21487338447175724\n",
      "Epoch 32, Training Loss: 0.006301357877759889, Validation Loss: 0.18387860229996278\n",
      "Epoch 33, Training Loss: 0.005752816035223112, Validation Loss: 0.236143660555863\n",
      "Epoch 34, Training Loss: 0.007846584834829255, Validation Loss: 0.21403814785232173\n",
      "Epoch 35, Training Loss: 0.006349710537664712, Validation Loss: 0.20296043311035328\n",
      "Epoch 36, Training Loss: 0.007585243152262764, Validation Loss: 0.17947907390807574\n",
      "Epoch 37, Training Loss: 0.00677450214898857, Validation Loss: 0.183976445576427\n",
      "Epoch 38, Training Loss: 0.004997461397845639, Validation Loss: 0.2206072942177238\n",
      "Epoch 39, Training Loss: 0.008717086390049918, Validation Loss: 0.21230507035807036\n",
      "Epoch 40, Training Loss: 0.005917152338057372, Validation Loss: 0.23292772009780263\n",
      "Epoch 41, Training Loss: 0.008060477855727858, Validation Loss: 0.19585759716382406\n",
      "Epoch 42, Training Loss: 0.00632434996802211, Validation Loss: 0.19032824615304159\n",
      "Epoch 43, Training Loss: 0.0056885463381498615, Validation Loss: 0.21927848952304302\n",
      "Epoch 44, Training Loss: 0.007345313555893329, Validation Loss: 0.26324199751499844\n",
      "Epoch 45, Training Loss: 0.00668020304437861, Validation Loss: 0.2267040546303958\n",
      "Epoch 46, Training Loss: 0.004432148117608326, Validation Loss: 0.23853293240456558\n",
      "Epoch 47, Training Loss: 0.0069999635706996235, Validation Loss: 0.21393100946498902\n",
      "Epoch 48, Training Loss: 0.007711025829842593, Validation Loss: 0.20593101055192287\n",
      "Epoch 49, Training Loss: 0.005346724664008596, Validation Loss: 0.27376693903354854\n",
      "Epoch 50, Training Loss: 0.00242801772223242, Validation Loss: 0.22308538344658801\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        return self.network(x)\n",
    "\n",
    "# Model instantiation for MNIST\n",
    "input_size = 28*28  \n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10 \n",
    "\n",
    "# usage for MNIST\n",
    "# Data loading and preprocessing\n",
    "dataset = MNIST(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = MLP(input_size, hidden_sizes, output_size) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Validation\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            \n",
    "            images = images.view(images.size(0), -1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bfc8c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.3304752808089602, Validation Loss: 0.16701227325470525\n",
      "Epoch 2, Training Loss: 0.1030746514094338, Validation Loss: 0.08519981270457835\n",
      "Epoch 3, Training Loss: 0.06965394085483494, Validation Loss: 0.07792720769004441\n",
      "Epoch 4, Training Loss: 0.05478341951713179, Validation Loss: 0.07124579050070408\n",
      "Epoch 5, Training Loss: 0.04624792418690049, Validation Loss: 0.05473621449671052\n",
      "Epoch 6, Training Loss: 0.037443981970772734, Validation Loss: 0.052599341150353404\n",
      "Epoch 7, Training Loss: 0.030259294712445872, Validation Loss: 0.05736102719187174\n",
      "Epoch 8, Training Loss: 0.026605457575488753, Validation Loss: 0.06313925595276622\n",
      "Epoch 9, Training Loss: 0.023263377666541917, Validation Loss: 0.06297933396318184\n",
      "Epoch 10, Training Loss: 0.02230519692158928, Validation Loss: 0.059148587802886315\n",
      "Epoch 11, Training Loss: 0.01803589461899593, Validation Loss: 0.0574143065557806\n",
      "Epoch 12, Training Loss: 0.015772132393258705, Validation Loss: 0.07723691984483594\n",
      "Epoch 13, Training Loss: 0.015567944416229197, Validation Loss: 0.059444645053517015\n",
      "Epoch 14, Training Loss: 0.013004659873787912, Validation Loss: 0.06370840151888349\n",
      "Epoch 15, Training Loss: 0.014354771952961837, Validation Loss: 0.05618183007120624\n",
      "Epoch 16, Training Loss: 0.011011894667775963, Validation Loss: 0.056646778040048765\n",
      "Epoch 17, Training Loss: 0.011012415293453483, Validation Loss: 0.06024942513177495\n",
      "Epoch 18, Training Loss: 0.010408780731562733, Validation Loss: 0.05629709465686941\n",
      "Epoch 19, Training Loss: 0.009969577269242871, Validation Loss: 0.060230644991300306\n",
      "Epoch 20, Training Loss: 0.00947987241831424, Validation Loss: 0.06854325832865528\n",
      "Epoch 21, Training Loss: 0.007855236951578184, Validation Loss: 0.06893880233435325\n",
      "Epoch 22, Training Loss: 0.007776531285103993, Validation Loss: 0.07063300157825678\n",
      "Epoch 23, Training Loss: 0.007062685893836151, Validation Loss: 0.07307848854903329\n",
      "Epoch 24, Training Loss: 0.007173027778144465, Validation Loss: 0.0552950157436403\n",
      "Epoch 25, Training Loss: 0.0074935862072515414, Validation Loss: 0.06466162571548086\n",
      "Epoch 26, Training Loss: 0.0064629643711600175, Validation Loss: 0.08553521870554354\n",
      "Epoch 27, Training Loss: 0.006637338090697759, Validation Loss: 0.0752299123766767\n",
      "Epoch 28, Training Loss: 0.006633673118130145, Validation Loss: 0.05933135866024728\n",
      "Epoch 29, Training Loss: 0.004530543346169155, Validation Loss: 0.07043137363207735\n",
      "Epoch 30, Training Loss: 0.00570442695854217, Validation Loss: 0.07280369954587328\n",
      "Epoch 31, Training Loss: 0.006159800734440528, Validation Loss: 0.07038778136687442\n",
      "Epoch 32, Training Loss: 0.004344938976125213, Validation Loss: 0.07903464728795802\n",
      "Epoch 33, Training Loss: 0.004852854841918309, Validation Loss: 0.06987040806808238\n",
      "Epoch 34, Training Loss: 0.005213430508547817, Validation Loss: 0.07218199606692907\n",
      "Epoch 35, Training Loss: 0.005169650905760742, Validation Loss: 0.0980769173990422\n",
      "Epoch 36, Training Loss: 0.004871873031498532, Validation Loss: 0.08088597108093064\n",
      "Epoch 37, Training Loss: 0.005173010922316829, Validation Loss: 0.10482698099016856\n",
      "Epoch 38, Training Loss: 0.0057467256033288195, Validation Loss: 0.07075227612862596\n",
      "Epoch 39, Training Loss: 0.002756837561969568, Validation Loss: 0.0963556011975299\n",
      "Epoch 40, Training Loss: 0.005285293533149888, Validation Loss: 0.1022297145526562\n",
      "Epoch 41, Training Loss: 0.00411018248871709, Validation Loss: 0.08766202952681226\n",
      "Epoch 42, Training Loss: 0.0048570429079898714, Validation Loss: 0.08146051893918509\n",
      "Epoch 43, Training Loss: 0.0036680307505603735, Validation Loss: 0.09024253922139937\n",
      "Epoch 44, Training Loss: 0.0032950386859506016, Validation Loss: 0.08568103506756608\n",
      "Epoch 45, Training Loss: 0.0044303464484246, Validation Loss: 0.08534099177874617\n",
      "Epoch 46, Training Loss: 0.003447794411217698, Validation Loss: 0.09304842434195455\n",
      "Epoch 47, Training Loss: 0.004668912063391936, Validation Loss: 0.07840866468048681\n",
      "Epoch 48, Training Loss: 0.002649306999184898, Validation Loss: 0.09240734421415797\n",
      "Epoch 49, Training Loss: 0.005003832271878992, Validation Loss: 0.12771662710293444\n",
      "Epoch 50, Training Loss: 0.0029449750189940163, Validation Loss: 0.10138632389870432\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),  \n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, stride=2),  \n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, stride=2)\n",
    "        )\n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Linear(256, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc_model(x)\n",
    "        return x\n",
    "\n",
    "dataset = MNIST(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = LeNet()  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Validation\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.14511992530353632, Validation Loss: 0.04739575870836872\n",
      "Epoch 2: Training Loss: 0.07340626115999935, Validation Loss: 0.06710829718534651\n",
      "Epoch 3: Training Loss: 0.058841791262323306, Validation Loss: 0.042739348355191754\n",
      "Epoch 4: Training Loss: 0.051676422281884106, Validation Loss: 0.04153339621272641\n",
      "Epoch 5: Training Loss: 0.041697825035294206, Validation Loss: 0.04126557059193555\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class FlexNet(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(FlexNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def train_model(dataset_class, input_channels):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    dataset = dataset_class(root='.', train=True, download=True, transform=transform)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = FlexNet(input_channels=input_channels)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Training Loss: {total_train_loss / len(train_loader)}, Validation Loss: {total_val_loss / len(val_loader)}')\n",
    "\n",
    "train_model(MNIST, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e71c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
